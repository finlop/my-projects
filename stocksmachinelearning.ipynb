{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\gargi\\anaconda3\\lib\\site-packages (0.2.38)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (2.31.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (4.9.3)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (2023.3.post1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (2.4.4)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (3.17.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (4.12.2)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fredapi in c:\\users\\gargi\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from fredapi) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from pandas->fredapi) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from pandas->fredapi) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from pandas->fredapi) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from pandas->fredapi) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->fredapi) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fredapi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gargi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\gargi\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk in c:\\users\\gargi\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\gargi\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\gargi\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for TSLA...\n",
      "Fetching data for AMD...\n",
      "Fetching data for SQ...\n",
      "Fetching data for ROKU...\n",
      "Fetching data for ZM...\n",
      "Fetching data for PTON...\n",
      "Fetching data for SPCE...\n",
      "Fetching data for MRNA...\n",
      "Fetching data for PLTR...\n",
      "Fetching data for PINS...\n",
      "Fetching data for BYND...\n",
      "Fetching data for DOCU...\n",
      "Fetching data for COIN...\n",
      "Fetching data for CRWD...\n",
      "Fetching data for NIO...\n",
      "Fetching data for ETSY...\n",
      "Fetching data for TWLO...\n",
      "Fetching data for W...\n",
      "Fetching data for ZG...\n",
      "Fetching data for FSLY...\n",
      "Fetching data for SHOP...\n",
      "Fetching data for BIDU...\n",
      "Fetching data for BABA...\n",
      "Fetching data for SPOT...\n",
      "Fetching data for U...\n",
      "Fetching data for TTD...\n",
      "Fetching data for OKTA...\n",
      "Fetching data for TEAM...\n",
      "Fetching data for DOCU...\n",
      "Fetching data for NET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HEXO: No timezone found, symbol may be delisted\n",
      "APHA: No timezone found, symbol may be delisted\n",
      "CLDR: No timezone found, symbol may be delisted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for TDOC...\n",
      "Fetching data for DDOG...\n",
      "Fetching data for SPCE...\n",
      "Fetching data for NKLA...\n",
      "Fetching data for CGC...\n",
      "Fetching data for HEXO...\n",
      "Fetching data for ACB...\n",
      "Fetching data for CRON...\n",
      "Fetching data for TLRY...\n",
      "Fetching data for APHA...\n",
      "Fetching data for SNDL...\n",
      "Fetching data for IRBT...\n",
      "Fetching data for CLDR...\n",
      "Fetching data for SNOW...\n",
      "Fetching data for ASAN...\n",
      "Fetching data for LMND...\n",
      "Fetching data for VEEV...\n",
      "Fetching data for MDB...\n",
      "Fetching data for FIL-USD...\n",
      "Fetching data for UNI7083-USD...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gargi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 1251.6829 - val_loss: 348.3125\n",
      "Epoch 2/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 99.1446 - val_loss: 278.3181\n",
      "Epoch 3/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 90.3724 - val_loss: 469.8478\n",
      "Epoch 4/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 78.2124 - val_loss: 433.3322\n",
      "Epoch 5/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 74.8309 - val_loss: 377.8325\n",
      "Epoch 6/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 77.1140 - val_loss: 554.5185\n",
      "Epoch 7/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 73.9349 - val_loss: 326.3806\n",
      "Epoch 8/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 70.4357 - val_loss: 268.1584\n",
      "Epoch 9/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 63.5983 - val_loss: 372.5905\n",
      "Epoch 10/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 66.2289 - val_loss: 478.1296\n",
      "Epoch 11/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 60.8486 - val_loss: 368.9952\n",
      "Epoch 12/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 56.4671 - val_loss: 419.2901\n",
      "Epoch 13/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 57.4298 - val_loss: 374.8130\n",
      "Epoch 14/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 59.9381 - val_loss: 704.6982\n",
      "Epoch 15/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 59.5492 - val_loss: 933.8149\n",
      "Epoch 16/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 58.6666 - val_loss: 459.4532\n",
      "Epoch 17/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 59.0735 - val_loss: 585.3077\n",
      "Epoch 18/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 54.4456 - val_loss: 283.0411\n",
      "Epoch 1/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 1577.2240 - val_loss: 1175.4689\n",
      "Epoch 2/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 282.3113 - val_loss: 1415.7222\n",
      "Epoch 3/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 278.1723 - val_loss: 1493.6829\n",
      "Epoch 4/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 244.5923 - val_loss: 2209.1367\n",
      "Epoch 5/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 245.6583 - val_loss: 2003.3718\n",
      "Epoch 6/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 224.0251 - val_loss: 2054.0344\n",
      "Epoch 7/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 215.1278 - val_loss: 1862.8481\n",
      "Epoch 8/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 221.5329 - val_loss: 2562.2646\n",
      "Epoch 9/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 196.6785 - val_loss: 2499.2217\n",
      "Epoch 10/100\n",
      "\u001b[1m426/426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 188.5430 - val_loss: 2782.9082\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "3-Day Model - MSE: 577.8060325463158, MAE: 10.117079158344147, R²: 0.9264864004737414\n",
      "10-Day Model - MSE: 4453.68851265525, MAE: 31.752493547171795, R²: 0.2843251663616575\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'days' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 241\u001b[0m\n\u001b[0;32m    236\u001b[0m ticker_predictions \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m--> 241\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict_future_prices_cnn(ticker, df, model_cnn_3d, model_cnn_10d, scaler, sequence_length, days)\n\u001b[0;32m    243\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m    244\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(df_ticker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], df_ticker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Prices\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'days' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Load Excel data\n",
    "df = pd.read_excel('final_stock_data.xlsx')\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('stock_data.db')\n",
    "\n",
    "# Save DataFrame to SQLite\n",
    "df.to_sql('stock_data', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for TSLA...\n",
      "Fetching data for AMD...\n",
      "Fetching data for SQ...\n",
      "Fetching data for ROKU...\n",
      "Fetching data for ZM...\n",
      "Fetching data for PTON...\n",
      "Fetching data for SPCE...\n",
      "Fetching data for MRNA...\n",
      "Fetching data for PLTR...\n",
      "Fetching data for PINS...\n",
      "Fetching data for BYND...\n",
      "Fetching data for DOCU...\n",
      "Fetching data for COIN...\n",
      "Fetching data for CRWD...\n",
      "Fetching data for NIO...\n",
      "Fetching data for ETSY...\n",
      "Fetching data for TWLO...\n",
      "Fetching data for W...\n",
      "Fetching data for ZG...\n",
      "Fetching data for FSLY...\n",
      "Fetching data for SHOP...\n",
      "Fetching data for BIDU...\n",
      "Fetching data for BABA...\n",
      "Fetching data for SPOT...\n",
      "Fetching data for U...\n",
      "Fetching data for TTD...\n",
      "Fetching data for OKTA...\n",
      "Fetching data for TEAM...\n",
      "Fetching data for DOCU...\n",
      "Fetching data for NET...\n",
      "Fetching data for TDOC...\n",
      "Fetching data for DDOG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HEXO: No timezone found, symbol may be delisted\n",
      "APHA: No timezone found, symbol may be delisted\n",
      "CLDR: No timezone found, symbol may be delisted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for SPCE...\n",
      "Fetching data for NKLA...\n",
      "Fetching data for CGC...\n",
      "Fetching data for HEXO...\n",
      "Fetching data for ACB...\n",
      "Fetching data for CRON...\n",
      "Fetching data for TLRY...\n",
      "Fetching data for APHA...\n",
      "Fetching data for SNDL...\n",
      "Fetching data for IRBT...\n",
      "Fetching data for CLDR...\n",
      "Fetching data for SNOW...\n",
      "Fetching data for ASAN...\n",
      "Fetching data for LMND...\n",
      "Fetching data for VEEV...\n",
      "Fetching data for MDB...\n",
      "Fetching data for FIL-USD...\n",
      "Fetching data for UNI7083-USD...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_cnn_model() missing 1 required positional argument: 'kernel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 146\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Create and train models for both 3-day and 10-day predictions\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m model_cnn_3d \u001b[38;5;241m=\u001b[39m create_cnn_model((X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m    147\u001b[0m model_cnn_10d \u001b[38;5;241m=\u001b[39m create_cnn_model((X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m    149\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: create_cnn_model() missing 1 required positional argument: 'kernel_size'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def fetch_and_save_data(stocks):\n",
    "    all_data = []\n",
    "\n",
    "    for symbol in stocks:\n",
    "        print(f\"Fetching data for {symbol}...\")\n",
    "        end_date = datetime.date.today()\n",
    "        start_date = end_date - datetime.timedelta(days=730)\n",
    "        stock = yf.Ticker(symbol)\n",
    "        \n",
    "        stock_data = stock.history(start=start_date, end=end_date)\n",
    "        if not stock_data.empty:\n",
    "            stock_data.reset_index(inplace=True)\n",
    "            stock_data['Symbol'] = symbol\n",
    "            stock_data['Dividends'] = stock.dividends.reindex(stock_data['Date'], fill_value=0)\n",
    "            stock_data['Stock Splits'] = stock.splits.reindex(stock_data['Date'], fill_value=0)\n",
    "            stock_data['Date'] = stock_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "            stock_data['Target_3d'] = stock_data['Close'].shift(-3)\n",
    "            stock_data['Target_10d'] = stock_data['Close'].shift(-10)\n",
    "            stock_data['Sequence'] = stock_data.groupby('Symbol').cumcount() + 1\n",
    "\n",
    "            all_data.append(stock_data)\n",
    "\n",
    "    df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Feature engineering\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "\n",
    "    def compute_rsi(series, window=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['SMA'] = df.groupby('Symbol')['Close'].rolling(window=14).mean().reset_index(level=0, drop=True)\n",
    "    df['RSI'] = df.groupby('Symbol')['Close'].apply(lambda x: compute_rsi(x)).reset_index(level=0, drop=True)\n",
    "    exp1 = df.groupby('Symbol')['Close'].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
    "    exp2 = df.groupby('Symbol')['Close'].transform(lambda x: x.ewm(span=26, adjust=False).mean())\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    df['BB_Middle'] = df.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=20).mean())\n",
    "    df['BB_Upper'] = df['BB_Middle'] + 2 * df.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=20).std())\n",
    "    df['BB_Lower'] = df['BB_Middle'] - 2 * df.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=20).std())\n",
    "\n",
    "    # Handle missing values appropriately\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Normalize the features\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.difference(['Close', 'Target_3d', 'Target_10d', 'Stock Splits'])\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "    # Select only the required columns\n",
    "    required_columns = [\n",
    "        'Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', \n",
    "        'Symbol', 'Target_3d', 'Target_10d', 'Sequence', 'Year', 'Month', 'Day', 'DayOfWeek', \n",
    "        'SMA', 'RSI', 'MACD', 'BB_Middle', 'BB_Upper', 'BB_Lower'\n",
    "    ]\n",
    "    df_scaled = df_scaled[required_columns]\n",
    "    \n",
    "    # Save the feature names\n",
    "    joblib.dump(numeric_columns, 'feature_names.pkl')\n",
    "    # Save the final cleaned and scaled data\n",
    "    df_scaled.to_excel('final_stock_data.xlsx', index=False)\n",
    "\n",
    "    return df, df_scaled, numeric_columns, scaler\n",
    "\n",
    "# Initialize stocks and fetch data\n",
    "stocks = [\"TSLA\", \"AMD\", \"SQ\", \"ROKU\", \"ZM\", \"PTON\", \"SPCE\", \"MRNA\", \"PLTR\", \"PINS\", \n",
    "          \"BYND\", \"DOCU\", \"COIN\", \"CRWD\", \"NIO\", \"ETSY\", \"TWLO\", \"W\", \"ZG\", \"FSLY\", \n",
    "          \"SHOP\", \"BIDU\", \"BABA\", \"SPOT\", \"U\", \"TTD\", \"OKTA\", \"TEAM\", \"DOCU\", \"NET\", \n",
    "          \"TDOC\", \"DDOG\", \"SPCE\", \"NKLA\", \"CGC\", \"HEXO\", \"ACB\", \"CRON\", \"TLRY\", \"APHA\", \n",
    "          \"SNDL\", \"IRBT\", \"CLDR\", \"SNOW\", \"ASAN\", \"LMND\", \"VEEV\", \"MDB\",\"FIL-USD\", \"UNI7083-USD\"]\n",
    "\n",
    "df, df_scaled, numeric_columns, scaler = fetch_and_save_data(stocks)\n",
    "\n",
    "# Create inputs for CNN (ticker-wise)\n",
    "tickers = df_scaled['Symbol'].unique()\n",
    "X, y_3d, y_10d = [], [], []\n",
    "\n",
    "sequence_length = 60  # Define the sequence length for the CNN\n",
    "\n",
    "for ticker in tickers:\n",
    "    df_ticker = df_scaled[df_scaled['Symbol'] == ticker].sort_values('Date')\n",
    "    ticker_data = df_ticker[numeric_columns].values\n",
    "    \n",
    "    # Ensure that the target values are referenced correctly and safely\n",
    "    if len(df_ticker) > sequence_length + 10:\n",
    "        for i in range(len(df_ticker) - sequence_length - 10):\n",
    "            X.append(ticker_data[i:i+sequence_length])\n",
    "            y_3d.append(df_ticker['Target_3d'].values[i+sequence_length])\n",
    "            y_10d.append(df_ticker['Target_10d'].values[i+sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y_3d = np.array(y_3d)\n",
    "y_10d = np.array(y_10d)\n",
    "\n",
    "# Ensure correct shape for model training\n",
    "X = X.reshape(-1, sequence_length, X.shape[-1])\n",
    "y_3d = y_3d.reshape(-1, 1)\n",
    "y_10d = y_10d.reshape(-1, 1)\n",
    "\n",
    "# Split data into train and test sets (e.g., 80-20 split)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train_3d, y_test_3d = y_3d[:split_index], y_3d[split_index:]\n",
    "y_train_10d, y_test_10d = y_10d[:split_index], y_10d[split_index:]\n",
    "\n",
    "def create_cnn_model(input_shape, kernel_size):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create and train models for both 3-day and 10-day predictions\n",
    "model_cnn_3d = create_cnn_model((X_train.shape[1], X_train.shape[2]))\n",
    "model_cnn_10d = create_cnn_model((X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train 3-day model\n",
    "history_3d = model_cnn_3d.fit(X_train, y_train_3d, epochs=50, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Train 10-day model\n",
    "history_10d = model_cnn_10d.fit(X_train, y_train_10d, epochs=50, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate models\n",
    "preds_cnn_3d = model_cnn_3d.predict(X_test).flatten()\n",
    "mse_3d = mean_squared_error(y_test_3d, preds_cnn_3d)\n",
    "mae_3d = mean_absolute_error(y_test_3d, preds_cnn_3d)\n",
    "r2_3d = r2_score(y_test_3d, preds_cnn_3d)\n",
    "\n",
    "preds_cnn_10d = model_cnn_10d.predict(X_test).flatten()\n",
    "mse_10d = mean_squared_error(y_test_10d, preds_cnn_10d)\n",
    "mae_10d = mean_absolute_error(y_test_10d, preds_cnn_10d)\n",
    "r2_10d = r2_score(y_test_10d, preds_cnn_10d)\n",
    "\n",
    "print(f'3-Day Model - MSE: {mse_3d}, MAE: {mae_3d}, R²: {r2_3d}')\n",
    "print(f'10-Day Model - MSE: {mse_10d}, MAE: {mae_10d}, R²: {r2_10d}')\n",
    "\n",
    "# Predict future prices\n",
    "def generate_future_dates(start_date, days):\n",
    "    return [start_date + datetime.timedelta(days=i) for i in range(1, days + 1)]\n",
    "\n",
    "def predict_future_prices_cnn(ticker, df, df_scaled, model_cnn_3d, model_cnn_10d, scaler, sequence_length=60, days=[3, 10]):\n",
    "    df_ticker = df[df['Symbol'] == ticker]\n",
    "    df_ticker_scaled = df_scaled[df_scaled['Symbol'] == ticker]\n",
    "    \n",
    "    X = df_ticker_scaled[numeric_columns].values\n",
    "    last_sequence = X[-sequence_length:]\n",
    "    \n",
    "    future_dates = generate_future_dates(df_ticker['Date'].max(), max(days))\n",
    "    predictions = {day: None for day in days}\n",
    "\n",
    "    prediction_3d = None\n",
    "    prediction_10d = None\n",
    "\n",
    "    for i in range(max(days)):\n",
    "        prediction_input = last_sequence.reshape(1, sequence_length, len(numeric_columns))\n",
    "\n",
    "        if i + 1 == 3:\n",
    "            prediction_3d = model_cnn_3d.predict(prediction_input)[0, 0]\n",
    "            predictions[3] = prediction_3d\n",
    "        if i + 1 == 10:\n",
    "            prediction_10d = model_cnn_10d.predict(prediction_input)[0, 0]\n",
    "            predictions[10] = prediction_10d\n",
    "\n",
    "        # Update last sequence with the prediction\n",
    "        last_sequence = np.roll(last_sequence, -1, axis=0)\n",
    "        if prediction_3d is not None and i < 3:\n",
    "            last_sequence[-1, 0] = prediction_3d\n",
    "        if prediction_10d is not None and i < 10:\n",
    "            last_sequence[-1, 0] = prediction_10d\n",
    "\n",
    "    # Inverse transform the predictions\n",
    "    for day in days:\n",
    "        if predictions[day] is not None:\n",
    "            predictions[day] = scaler.inverse_transform(\n",
    "                np.concatenate((np.array(predictions[day]).reshape(1, 1), \n",
    "                                np.zeros((1, len(numeric_columns) - 1))), axis=1)\n",
    "            )[0, 0]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Predict for each ticker\n",
    "ticker_predictions = {}\n",
    "days = [3, 10]\n",
    "for ticker in df['Symbol'].unique():\n",
    "    predictions = predict_future_prices_cnn(ticker, df, df_scaled, model_cnn_3d, model_cnn_10d, scaler, sequence_length=60, days=days)\n",
    "    \n",
    "    df_ticker = df[df['Symbol'] == ticker]\n",
    "    future_dates = generate_future_dates(df_ticker['Date'].max(), max(days))\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(df_ticker['Date'], df_ticker['Close'], label='Actual Prices')\n",
    "    for day in days:\n",
    "        plt.scatter(future_dates[day-1], predictions[day], color='red', zorder=5, label=f'{day}-Day Prediction: {predictions[day]:.2f}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(f'Future Stock Price Prediction for {ticker}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
