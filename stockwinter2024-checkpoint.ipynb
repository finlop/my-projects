{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2d7cd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification workflow required. Please check your Robinhood app for instructions.\n",
      "Waiting for challenge to be validated\n",
      "5.0898966789245605\n",
      "Waiting for challenge to be validated\n",
      "10.173388957977295\n",
      "Waiting for challenge to be validated\n",
      "15.25538444519043\n",
      "Waiting for challenge to be validated\n",
      "20.343173027038574\n",
      "Waiting for challenge to be validated\n",
      "25.43269681930542\n",
      "Waiting for challenge to be validated\n",
      "30.520169973373413\n",
      "Waiting for challenge to be validated\n",
      "35.598580837249756\n",
      "Waiting for challenge to be validated\n",
      "40.684181451797485\n",
      "Waiting for challenge to be validated\n",
      "45.83382701873779\n",
      "✅ Token saved to robinhood_token.txt\n"
     ]
    }
   ],
   "source": [
    "from robinhood_auth_login import login\n",
    "\n",
    "# Perform login\n",
    "login_response = login(\n",
    "    username=\"nandarohan442@gmail.com\",\n",
    "    password=\"Ron1506313015$\",\n",
    "    by_sms=True,\n",
    "    store_session=False\n",
    ")\n",
    "\n",
    "# Save token to file\n",
    "with open(\"robinhood_token.txt\", \"w\") as f:\n",
    "    f.write(login_response[\"access_token\"])\n",
    "\n",
    "print(\"✅ Token saved to robinhood_token.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a5db4-b329-4583-ae8c-764cc000f179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e84ac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock data saved to my_stock_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Load token from file\n",
    "def load_token_from_file(file_path=\"robinhood_token.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    return None\n",
    "\n",
    "# Fetch and save stock data\n",
    "def fetch_and_save_stock_data(token, output_file=\"my_stock_data.xlsx\"):\n",
    "    url = \"https://api.robinhood.com/positions/\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error fetching stock data:\", response.text)\n",
    "        return\n",
    "\n",
    "    positions = response.json().get(\"results\", [])\n",
    "    stock_data = []\n",
    "\n",
    "    # Get the current timestamp\n",
    "    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    for position in positions:\n",
    "        if float(position[\"quantity\"]) > 0:  # Only fetch open positions\n",
    "            instrument_url = position[\"instrument\"]\n",
    "            instrument_data = requests.get(instrument_url, headers=headers).json()\n",
    "\n",
    "            symbol = instrument_data[\"symbol\"]\n",
    "            quantity = float(position[\"quantity\"])\n",
    "            average_buy_price = float(position[\"average_buy_price\"])\n",
    "            quote_url = f\"https://api.robinhood.com/quotes/{symbol}/\"\n",
    "            current_price = float(requests.get(quote_url, headers=headers).json()[\"last_trade_price\"])\n",
    "\n",
    "            market_value = current_price * quantity\n",
    "            profit_loss = (current_price - average_buy_price) * quantity\n",
    "\n",
    "            stock_data.append({\n",
    "                \"Timestamp\": fetch_time,  # Add the timestamp\n",
    "                \"Symbol\": symbol,\n",
    "                \"Quantity\": quantity,\n",
    "                \"Average Buy Price\": average_buy_price,\n",
    "                \"Current Price\": current_price,\n",
    "                \"Market Value\": market_value,\n",
    "                \"Profit/Loss\": profit_loss\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(stock_data)\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Stock data saved to {output_file}\")\n",
    "\n",
    "# Use token to fetch data\n",
    "token = load_token_from_file()\n",
    "if token:\n",
    "    fetch_and_save_stock_data(token)\n",
    "else:\n",
    "    print(\"No token found. Please log in first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56733c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all tickers from Robinhood...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Function to fetch all tickers from Robinhood\n",
    "def fetch_all_tickers_from_robinhood():\n",
    "    classification_map = {\n",
    "        \"stock\": \"Equity\",\n",
    "        \"etf\": \"Exchange-Traded Fund\",\n",
    "        \"warrant\": \"Derivative\",\n",
    "        \"mutual_fund\": \"Mutual Fund\",\n",
    "        \"bond\": \"Fixed Income\",\n",
    "    }\n",
    "    sector_classifications = {\n",
    "        \"software\": \"Technology\",\n",
    "        \"hardware\": \"Technology\",\n",
    "        \"semiconductors\": \"Technology\",\n",
    "        \"it services\": \"Technology\",\n",
    "        \"pharmaceuticals\": \"Healthcare\",\n",
    "        \"biotech\": \"Healthcare\",\n",
    "        \"medical devices\": \"Healthcare\",\n",
    "        \"healthcare services\": \"Healthcare\",\n",
    "        \"banks\": \"Financials\",\n",
    "        \"insurance companies\": \"Financials\",\n",
    "        \"investment firms\": \"Financials\",\n",
    "        \"reits\": \"Real Estate\",\n",
    "        \"retail\": \"Consumer Discretionary\",\n",
    "        \"automotive\": \"Consumer Discretionary\",\n",
    "        \"travel\": \"Consumer Discretionary\",\n",
    "        \"entertainment\": \"Consumer Discretionary\",\n",
    "        \"food\": \"Consumer Staples\",\n",
    "        \"beverages\": \"Consumer Staples\",\n",
    "        \"personal products\": \"Consumer Staples\",\n",
    "        \"household goods\": \"Consumer Staples\",\n",
    "        \"oil\": \"Energy\",\n",
    "        \"gas\": \"Energy\",\n",
    "        \"renewable energy\": \"Energy\",\n",
    "        \"energy equipment\": \"Energy\",\n",
    "        \"manufacturing\": \"Industrials\",\n",
    "        \"construction\": \"Industrials\",\n",
    "        \"aerospace\": \"Industrials\",\n",
    "        \"logistics\": \"Industrials\",\n",
    "        \"mining\": \"Materials\",\n",
    "        \"chemicals\": \"Materials\",\n",
    "        \"construction materials\": \"Materials\",\n",
    "        \"forestry products\": \"Materials\",\n",
    "        \"electricity\": \"Utilities\",\n",
    "        \"water\": \"Utilities\",\n",
    "        \"natural gas\": \"Utilities\",\n",
    "        \"renewable utilities\": \"Utilities\",\n",
    "        \"commercial reits\": \"Real Estate\",\n",
    "        \"residential reits\": \"Real Estate\",\n",
    "        \"property management\": \"Real Estate\",\n",
    "        \"telecommunications\": \"Communication Services\",\n",
    "        \"media\": \"Communication Services\",\n",
    "        \"internet services\": \"Communication Services\",\n",
    "        \"cloud services\": \"Technology\",\n",
    "        \"ai\": \"Technology\"\n",
    "    }\n",
    "    url = \"https://api.robinhood.com/instruments/\"\n",
    "    tickers = []\n",
    "    next_url = url\n",
    "\n",
    "    while next_url:\n",
    "        try:\n",
    "            response = requests.get(next_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            tickers.extend([\n",
    "                {\n",
    "                    \"Ticker\": item['symbol'],\n",
    "                    \"Name\": item['name'],\n",
    "                    \"Type\": item['type'],\n",
    "                    \"Classification\": classification_map.get(item['type'], \"Unknown\"),\n",
    "                    \"Sector\": sector_classifications.get(item['name'].lower(), \"Unknown\"),\n",
    "                    \"Tradeable\": item['tradeable']\n",
    "                } for item in data.get('results', [])\n",
    "            ])\n",
    "            next_url = data.get('next')  # Handle pagination if available\n",
    "            if next_url:\n",
    "                time.sleep(1)  # Avoid hitting rate limits\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching tickers: {e}\")\n",
    "            break\n",
    "\n",
    "    return tickers\n",
    "\n",
    "# Save tickers to an Excel file\n",
    "def save_tickers_to_excel(tickers, filename=None):\n",
    "    if filename is None:\n",
    "        downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "        filename = os.path.join(downloads_path, \"robinhood_tickers.xlsx\")\n",
    "    df = pd.DataFrame(tickers)\n",
    "    df = df[df['Tradeable'] == True]  # Filter out non-tradable tickers\n",
    "    df.to_excel(filename, index=False)\n",
    "    print(f\"Tickers saved to {filename}\")\n",
    "\n",
    "# Main logic\n",
    "def main():\n",
    "    print(\"Fetching all tickers from Robinhood...\")\n",
    "    tickers = fetch_all_tickers_from_robinhood()\n",
    "\n",
    "    if not tickers:\n",
    "        print(\"No tickers fetched. Exiting...\")\n",
    "        return\n",
    "\n",
    "    save_tickers_to_excel(tickers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install yfinance pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b536b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "# Load tickers from Excel\n",
    "def load_tickers_from_excel(file_path):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df['Ticker'].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tickers from Excel: {e}\")\n",
    "        return []\n",
    "\n",
    "# Clean tickers list\n",
    "def clean_tickers(tickers):\n",
    "    return [str(ticker).strip().upper() for ticker in tickers if isinstance(ticker, str) and ticker.strip()]\n",
    "\n",
    "# Validate tickers in batches\n",
    "def validate_tickers_in_batches(tickers, batch_size=50):\n",
    "    valid_tickers = []\n",
    "    invalid_tickers = []\n",
    "    \n",
    "    for i in range(0, len(tickers), batch_size):\n",
    "        batch = tickers[i:i + batch_size]\n",
    "        print(f\"Validating batch {i // batch_size + 1} of {len(tickers) // batch_size + 1}...\")\n",
    "        for ticker in batch:\n",
    "            try:\n",
    "                stock = yf.Ticker(ticker)\n",
    "                if not stock.history(period=\"1d\").empty:  # Check for data availability\n",
    "                    valid_tickers.append(ticker)\n",
    "                else:\n",
    "                    invalid_tickers.append(ticker)\n",
    "            except Exception as e:\n",
    "                invalid_tickers.append(ticker)\n",
    "                print(f\"Validation failed for {ticker}: {e}\")\n",
    "        time.sleep(2)  # Pause to avoid rate limits\n",
    "\n",
    "    print(f\"Valid tickers: {len(valid_tickers)}, Invalid tickers: {len(invalid_tickers)}\")\n",
    "    return valid_tickers, invalid_tickers\n",
    "\n",
    "# Save filtered tickers to an Excel file\n",
    "def save_filtered_tickers(valid_tickers, invalid_tickers, output_file):\n",
    "    valid_df = pd.DataFrame({'Valid Tickers': valid_tickers})\n",
    "    invalid_df = pd.DataFrame({'Invalid Tickers': invalid_tickers})\n",
    "\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        valid_df.to_excel(writer, sheet_name='Valid Tickers', index=False)\n",
    "        invalid_df.to_excel(writer, sheet_name='Invalid Tickers', index=False)\n",
    "\n",
    "    print(f\"Filtered tickers saved to {output_file}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    input_file = \"robinhood_tickers.xlsx\"  # Input file with tickers\n",
    "    output_file = \"validated_tickers.xlsx\"  # Output file for validated tickers\n",
    "\n",
    "    print(\"Loading tickers from Excel...\")\n",
    "    tickers = load_tickers_from_excel(input_file)\n",
    "\n",
    "    if not tickers:\n",
    "        print(\"No tickers found. Exiting...\")\n",
    "        return\n",
    "\n",
    "    print(\"Cleaning tickers list...\")\n",
    "    tickers = clean_tickers(tickers)\n",
    "\n",
    "    if not tickers:\n",
    "        print(\"No valid tickers found after cleaning. Exiting...\")\n",
    "        return\n",
    "\n",
    "    print(\"Validating tickers in batches...\")\n",
    "    valid_tickers, invalid_tickers = validate_tickers_in_batches(tickers)\n",
    "\n",
    "    print(\"Saving filtered tickers...\")\n",
    "    save_filtered_tickers(valid_tickers, invalid_tickers, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282b90a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9d7d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import yfinance.shared\n",
    "\n",
    "# Create a custom session with a timeout\n",
    "def set_request_timeout(timeout=10):\n",
    "    session = requests.Session()\n",
    "    session.request = lambda *args, **kwargs: requests.Session.request(session, *args, timeout=timeout, **kwargs)\n",
    "    yfinance.shared._session = session\n",
    "\n",
    "# Set the timeout for requests\n",
    "set_request_timeout(10)\n",
    "\n",
    "# Load validated tickers from Excel\n",
    "def load_validated_tickers(file_path):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name='Valid Tickers')\n",
    "        return df['Valid Tickers'].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tickers from Excel: {e}\")\n",
    "        return []\n",
    "\n",
    "# Batch download historical data in smaller chunks with retry logic and delays\n",
    "def batch_download(tickers, period=\"6mo\", batch_size=50, max_retries=3):  # Changed period to 6 months\n",
    "    all_data = []\n",
    "    total_batches = len(tickers) // batch_size + (1 if len(tickers) % batch_size > 0 else 0)\n",
    "    failed_tickers = []\n",
    "\n",
    "    for i in range(0, len(tickers), batch_size):\n",
    "        batch = tickers[i:i + batch_size]\n",
    "        print(f\"Downloading batch {i // batch_size + 1} of {total_batches}...\")\n",
    "        batch_failures = []\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                data = yf.download(batch, period=period, group_by=\"ticker\", threads=True, progress=False)\n",
    "                all_data.append(data)\n",
    "                break  # Exit retry loop on success\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed for batch {i // batch_size + 1}: {e}\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                if attempt == max_retries - 1:  # If max retries reached\n",
    "                    batch_failures.extend(batch)\n",
    "        print(f\"Batch {i // batch_size + 1} completed: {len(batch) - len(batch_failures)} successful, {len(batch_failures)} failed.\")\n",
    "        failed_tickers.extend(batch_failures)\n",
    "        time.sleep(5)  # Delay between batches\n",
    "\n",
    "    if failed_tickers:\n",
    "        print(f\"Failed to download data for {len(failed_tickers)} tickers.\")\n",
    "        save_failed_tickers(failed_tickers)\n",
    "\n",
    "    if all_data:\n",
    "        return pd.concat(all_data, axis=1)\n",
    "    else:\n",
    "        print(\"No data downloaded.\")\n",
    "        return None\n",
    "\n",
    "# Save failed tickers to Excel\n",
    "def save_failed_tickers(failed_tickers, file_name=\"failed_tickers.xlsx\"):\n",
    "    pd.DataFrame({'Failed Tickers': failed_tickers}).to_excel(file_name, index=False)\n",
    "    print(f\"Failed tickers saved to {file_name}\")\n",
    "\n",
    "# Filter tickers by liquidity\n",
    "def filter_by_liquidity(data, min_volume=100000, min_dollar_volume=1000000):\n",
    "    filtered_tickers = []\n",
    "    for ticker in data.columns.get_level_values(0).unique():\n",
    "        try:\n",
    "            hist = data[ticker]\n",
    "            avg_volume = hist['Volume'].mean()\n",
    "            avg_dollar_volume = (hist['Close'] * hist['Volume']).mean()\n",
    "\n",
    "            if avg_volume >= min_volume and avg_dollar_volume >= min_dollar_volume:\n",
    "                filtered_tickers.append(ticker)\n",
    "        except Exception as e:\n",
    "            print(f\"Liquidity check failed for {ticker}: {e}\")\n",
    "\n",
    "    print(f\"Tickers after liquidity check: {len(filtered_tickers)}\")\n",
    "    return filtered_tickers\n",
    "\n",
    "# Filter tickers by volatility\n",
    "def filter_by_volatility(data, max_volatility=0.05):\n",
    "    filtered_tickers = []\n",
    "    for ticker in data.columns.get_level_values(0).unique():\n",
    "        try:\n",
    "            hist = data[ticker]\n",
    "            returns = hist['Close'].pct_change()\n",
    "            avg_volatility = returns.std()\n",
    "\n",
    "            if avg_volatility <= max_volatility:\n",
    "                filtered_tickers.append(ticker)\n",
    "        except Exception as e:\n",
    "            print(f\"Volatility check failed for {ticker}: {e}\")\n",
    "\n",
    "    print(f\"Tickers after volatility check: {len(filtered_tickers)}\")\n",
    "    return filtered_tickers\n",
    "\n",
    "# Additional filter for average trade frequency\n",
    "def filter_by_trade_frequency(data, min_trade_frequency=1000):\n",
    "    filtered_tickers = []\n",
    "    for ticker in data.columns.get_level_values(0).unique():\n",
    "        try:\n",
    "            hist = data[ticker]\n",
    "            trade_counts = hist.resample('1D').size()\n",
    "            avg_trade_freq = trade_counts.mean()\n",
    "\n",
    "            if avg_trade_freq >= min_trade_frequency:\n",
    "                filtered_tickers.append(ticker)\n",
    "        except Exception as e:\n",
    "            print(f\"Trade frequency check failed for {ticker}: {e}\")\n",
    "\n",
    "    print(f\"Tickers after trade frequency check: {len(filtered_tickers)}\")\n",
    "    return filtered_tickers\n",
    "\n",
    "# Save filtered tickers to Excel\n",
    "def save_filtered_tickers(filtered_tickers, output_file):\n",
    "    filtered_df = pd.DataFrame({'Filtered Tickers': filtered_tickers})\n",
    "\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        filtered_df.to_excel(writer, sheet_name='Filtered Tickers', index=False)\n",
    "\n",
    "    print(f\"Filtered tickers saved to {output_file}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    input_file = \"validated_tickers.xlsx\"  # Input file with validated tickers\n",
    "    output_file = \"filtered_tickers.xlsx\"  # Output file for filtered tickers\n",
    "    period = \"6mo\"  # Changed period to 6 months\n",
    "    batch_size = 50  # Number of tickers per batch\n",
    "\n",
    "    print(\"Loading validated tickers...\")\n",
    "    tickers = load_validated_tickers(input_file)\n",
    "\n",
    "    if not tickers:\n",
    "        print(\"No validated tickers found. Exiting...\")\n",
    "        return\n",
    "\n",
    "    print(\"Downloading historical data for tickers in batches...\")\n",
    "    data = batch_download(tickers, period=period, batch_size=batch_size)\n",
    "\n",
    "    if data is None:\n",
    "        print(\"Failed to download data. Exiting...\")\n",
    "        return\n",
    "\n",
    "    print(\"Filtering by liquidity...\")\n",
    "    liquid_tickers = filter_by_liquidity(data)\n",
    "\n",
    "    print(\"Filtering by volatility...\")\n",
    "    volatility_filtered_tickers = filter_by_volatility(data[liquid_tickers])\n",
    "\n",
    "    print(\"Filtering by average trade frequency...\")\n",
    "    final_tickers = filter_by_trade_frequency(data[volatility_filtered_tickers])\n",
    "\n",
    "    print(\"Saving filtered tickers...\")\n",
    "    save_filtered_tickers(final_tickers, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9201a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the filtered tickers from the Excel file\n",
    "file_path = \"filtered_tickers.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(file_path, sheet_name='Filtered Tickers')\n",
    "    \n",
    "    # Count the number of tickers\n",
    "    ticker_count = df.shape[0]\n",
    "    \n",
    "    # Display the result\n",
    "    print(f\"Total number of filtered tickers: {ticker_count}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {file_path} was not found. Please make sure the file exists in the correct directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c758baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a54e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8e876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880b0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4801b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4040ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a5489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b2f5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
