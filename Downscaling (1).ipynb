{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caaa72f5",
   "metadata": {},
   "source": [
    "# GEOS 5314 Exercise 11: Introduction to Downscaling (local bias correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68be82",
   "metadata": {},
   "source": [
    "This notebook gives an introduction into local bias correction, generally referred to as \"statistical downscaling\".  Intended to run in custon conda environment *geoClimate*.  If loading additional packages, be sure they have been installed using \"conda install -c conda-forge packagename\".\n",
    "This Notebook is licensed for free and open consumption under the [Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/) license.\n",
    "\n",
    "CVS: $Id: Downscaling.ipynb,v 25.4 2025/04/08 01:18:56 brikowi Exp $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load needed packages\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request              # Manage URL's\n",
    "import numpy as np                 # Basic math and simple arrays\n",
    "import xarray as xr                # Better & faster multidimensional arrays\n",
    "import zarr                        # Chunked very fast cloud-based arrays\n",
    "from sklearn.linear_model import LinearRegression  # Use conda install -c conda-forge scikit-learn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import pandas as pd                # Dataframes, similar to internal spreadsheet\n",
    "from difflib import get_close_matches\n",
    "import gcsfs                       # Google cloud filesystem routines\n",
    "import datetime\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d44224",
   "metadata": {},
   "source": [
    "## Get list of models available at Google\n",
    "Get file list from Google via the given URL, use Pandas (pd) to read the CSV file and save in a *dataframe* \"df\".  A Pandas dataframe is like a spreadsheet format for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e9807",
   "metadata": {},
   "source": [
    "## Get list of all available SSP3 7.0 models with *tasmax*\n",
    "Find lines in the file-list dataframe \"df\" that meet the given criteria, most important are that the variable *tasmax* is in the file and that it is from a model of experiment *ssp370* (the currently most-likely CO2 emissions scenario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7150da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssp370 = df.query(\"activity_id=='ScenarioMIP' & table_id == 'Amon' & \" +\\\n",
    "    \"variable_id == 'tasmax' & experiment_id == 'ssp370' & member_id == 'r1i1p1f1'\")\n",
    "print(len(df_ssp370), 'forecast files match the search criteria, the first 3 are:')\n",
    "df_ssp370.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22317d",
   "metadata": {},
   "source": [
    "## Get list of all historical models with *tasmax*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12582277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_historical = df.query(\"activity_id == 'CMIP' & table_id == 'Amon' & \" +\\\n",
    "    \"variable_id == 'tasmax' & experiment_id == 'historical' & member_id == 'r1i1p1f1'\")\n",
    "print(len(df_historical), 'historical files match the search criteria, the first 3 are:')\n",
    "df_historical.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb681d",
   "metadata": {},
   "source": [
    "## Make list of models with BOTH historical and future tasmax\n",
    "See <a href=\"https://stackoverflow.com/questions/55898796/how-to-match-keys-of-2-data-frames-and-create-new-df-with-matching-keys\">StackOverflow</a> for suggestions.  Current code requires start with shortest dataframe (!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ad019",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [r for r in df_ssp370[\"source_id\"] if get_close_matches(r, df_historical[\"source_id\"], n=1, cutoff = .85)]\n",
    "bothSources = np.unique(np.array(seq))\n",
    "print(seq)\n",
    "\n",
    "print(len(bothSources), \"models have both historical and SSP3-7.0 results\")\n",
    "print(\"e.g. \", bothSources[3])\n",
    "\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bothSources)):    # model = 'GFDL-CM4'\n",
    "    print(\"Model %d : %s\" % (i, bothSources[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c7e22",
   "metadata": {},
   "source": [
    "## Define functions to get desired variable from desired model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVar (modelList, modelName):\n",
    "    # Custom function to get variable for model 'modelName' from list of possible models 'modelList'\n",
    "    zstore = modelList.query(f\"source_id == '{modelName}'\").zstore.values[0]\n",
    "    ds = xr.open_zarr(zstore, consolidated = True)\n",
    "    ds.load()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lat-long point for evaluation, DFW is default here\n",
    "locName = 'DFW'                     # Name of location (for labeling plots)\n",
    "pointLat = 32.7666\n",
    "pointLon = 360 - 96.7778   # Longitude must be positive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d4120",
   "metadata": {},
   "source": [
    "# Compute RMSE of historical model vs. PRISM-observed T<sub>max</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8fdcee",
   "metadata": {},
   "source": [
    "## Read local observed T<sub>max</sub> from downloaded PRISM file\n",
    "\n",
    "Use the file for you local area downloaded from [PRISM explorer](https://prism.oregonstate.edu/explorer/).  Rename that file '*PRISMobserved_1950-2014.csv*' (or change the variable *PRISMfile* in the next cell), and be sure the last header line contains '*Date*' and '*tmean (degrees C)*'.  The file must be in the directory from which you started this Notebook, or give the full path to the file in *PRISMfile*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRISMfile = \"PRISMobserved_1950-2014.csv\"\n",
    "# PRISMfile = \"/home/brikowi/Teaching/Resilience/Exercises/Downscaling/PRISM_ppt_tmean_tmax_stable_4km_195001_201412_32.7666_-96.7778.csv\"\n",
    "obsDF = pd.read_csv(PRISMfile, skiprows=10)\n",
    "if (len(obsDF) != 780):\n",
    "    print(\"PRISM file needs monthly data for years 1950-2014, i.e. 780 entries.  Your file has %d entries\" %\n",
    "         len(obsDF) )\n",
    "    print(\"Aborting.  Please provide the correct file\")\n",
    "obsDF = obsDF.rename(columns={'tmean (degrees C)': 'tmean', 'tmax (degrees C)': 'tmax'})\n",
    "obsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d014f8fc",
   "metadata": {},
   "source": [
    "## Compute 100 quantiles of observed T<sub>max</sub>\n",
    "These are used to force (downscale for bias removal) the historical model T<sub>max</sub> to have the same distribution about the median T<sub>max</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_quantiles = obsDF.tmax.quantile(np.arange(0.01, 1, 0.01))\n",
    "obs_quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06204cc7",
   "metadata": {},
   "source": [
    "### Choose a single model for downscaling at your selected point\n",
    "Model chosen at random from those with historical and future results for ssp370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# model = bothSources[random.randint(0,len(bothSources))]      # Choose random model\n",
    "model = 'GFDL-ESM4'                                            # Select model with known serious bias\n",
    "print(\"Processing model: %s\" % (model), end='...')\n",
    "ds_hist = getVar(df_historical, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29612c3e",
   "metadata": {},
   "source": [
    "### Adjust times to be consistent between the datasets, for proper interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ffd289",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.to_datetime(datetime.date(1850,1,15)) # I chose 15 for all dates to make it easier.\n",
    "time_new_hist = [start_time + pd.DateOffset(months = x) for x in range(len(ds_hist.time))]\n",
    "ds_hist = ds_hist.assign_coords(time = time_new_hist)\n",
    "\n",
    "start_date = pd.to_datetime(datetime.date(1950,1,1))\n",
    "ds_hist_sel = ds_hist.isel(time=(ds_hist.time >= start_date))\n",
    "pointHist = ds_hist_sel.sel(lat=pointLat, lon=pointLon, method=\"nearest\").tasmax - 273.15 # Convert to Celsius\n",
    "histDF = pointHist.to_dataframe().drop({'height','lat','lon'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486c319",
   "metadata": {},
   "source": [
    "Slightly trickier for the observed dataset, which doesn't have CMIP-compatible dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xObs = xr.DataArray(obsDF.tmax.values,coords={'time':pointHist[\"time\"].values},\n",
    "                    dims=['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0870a2",
   "metadata": {},
   "source": [
    "### Compute same quantiles for the historical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_quantiles = histDF.tasmax.quantile(np.arange(0.01, 1, 0.01))\n",
    "hist_quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad6e00",
   "metadata": {},
   "source": [
    "# Make linear regression between the model and observed T<sub>max</sub>\n",
    "This is *Quantile Mapping*, the simplest of the downscaling methods (for bias, AKA error adustment).  We use linear regression to derive an equation that adjusts the modeled quantiles to agree with historical modeled quantiles.  The regression line plotted below is effectively a 1:1 fit between those two sets of quantiles.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression between the quantiles of the observed and simulated data\n",
    "reg = LinearRegression()\n",
    "reg.fit(hist_quantiles.to_frame(), obs_quantiles.to_frame())\n",
    "# Resorted to extreme treatment to avoid error\n",
    "#  DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. \n",
    "#  Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
    "slope = reg.coef_[0].squeeze().item()\n",
    "intercept = reg.intercept_[0]\n",
    "print(\"The regression equation is: downscaled_Tmax = %7.4f*modeled_Tmax + %7.4f\" % (slope, intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ebb13",
   "metadata": {},
   "source": [
    "## Plot the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15725aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(obs_quantiles.to_frame(), hist_quantiles.to_frame(),  color='black')\n",
    "plt.plot(hist_quantiles.to_frame(), reg.predict(hist_quantiles.to_frame()), color='blue', linewidth=3, label=\"Regression\")\n",
    "plt.legend(fontsize=13)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.ylabel('Historical Model Tmax', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Observed Tmax (degC)', fontsize=13, fontweight='bold')\n",
    "plt.title(f'Quantile Mapping, Historical vs. Observed Tmax: {model}',\n",
    "          fontsize=15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd2f59",
   "metadata": {},
   "source": [
    "# Compute downscaled T<sub>max</sub>\n",
    "This is really just bias correction at a single lat-long point.  The black dots above are forced onto the blue line by the regression formula (reg.predict).  Complete statistical downscaling would interpolate coarse GCM output to a finer grid then apply this or similar technique at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downscale the data (output is numpy.array)\n",
    "downscaled_data = reg.predict(histDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f2be8",
   "metadata": {},
   "source": [
    "## Compute RMSE \n",
    "Use simple machine learning tools to see if model fit was improved by bias correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d17ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rmse  = mean_squared_error(y_true=obsDF.tmax, y_pred=histDF)\n",
    "downscaled_rmse = mean_squared_error(y_true=histDF, y_pred=downscaled_data)\n",
    "print(\"RMSE between historical model and observed: %7.4f, and downscaled and observed: %7.4f\" % \n",
    "      (raw_rmse, downscaled_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b466e",
   "metadata": {},
   "source": [
    "# Plot the datasets\n",
    "Observed (PRISM), uncorrected GCM historical model, and downscaled (bias-corrected) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GCM and Downscaled to xarray for export to file and/or plotting\n",
    "xGCM = histDF.to_xarray()\n",
    "xDScal = xr.DataArray(downscaled_data.reshape(len(downscaled_data)), coords={'time':histDF.index}, \n",
    "                      dims={\"time\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f63bd8-0363-46dc-8875-32374717ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble as Pandas dataframe aggregated and indexed by year for easy export. Probably not the most efficient\n",
    "tempX1 = xGCM.groupby('time.year').mean().tasmax\n",
    "pdGCM = tempX1.to_pandas()\n",
    "tempX2 = xDScal.groupby('time.year').mean()\n",
    "pdDownscale = tempX2.to_pandas()\n",
    "tempX3 = xObs.groupby('time.year').mean()\n",
    "pdPRISM = tempX3.to_pandas()\n",
    "frames = [pdGCM, pdDownscale, pdPRISM]\n",
    "toExport = pd.concat(frames, axis=1)\n",
    "\n",
    "toExport.rename(columns={'tasmax': 'rawGCM', 0: 'downscaleGCM', 1: 'PRISM'}, inplace=True)\n",
    "toExport.to_csv('downscalingPlotData.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccbbe4-7ddb-47e9-93fe-2e3b92a7b571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
